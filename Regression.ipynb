{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 (38 marks total)\n",
    "### Due: March 8, 2024, at 11:59pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global rules: you are allowed to add as many code and markdown cells as you would like (and you probably should add some). Your are _not_ allowed to use `scikit-learn` or any other machine learning libraries. There should be enough comments and markdown cells explaining code that it is easy for a reader to understand your code and its outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will likely want to know about the function `np.linalg.pinv`, which implements the matrix pseudo-inverse, and that the symbol '`@`' is used by numpy to represent matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Linear Regression (21 marks total)\n",
    "\n",
    "In this part, implement linear regression using the closed-form solution for the optimal weights that we saw in class. Next, implement linear regression by finding weights through stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you begin, which of your implementations do you think will perform better? Why? Answer this question in the markdown cell below. Note: there is not a correct answer to this question, your evaluation will be based on your explanation (2 marks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think the closed form solution will perform better since it requires a single set of matrix operations to get a perfect solution. The stochastic gradient descent algorithm has simpler calculations to perform, but they may need to be performed thousands of times to approach the accuracy of the closed form solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put your implementations below. I have included imports you will almost certainly want, as well as seeded numpy's random number generator to ensure that I get the same output as you when marking. (8 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m \n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m \n\u001b[0;32m      4\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def linear_regression_closed_form(X, t):\n",
    "    X = np.concatenate((np.ones(X.shape[0], int)[np.newaxis, :].T, X), axis=1)\n",
    "    w = np.linalg.pinv(X) @ t\n",
    "\n",
    "    def predict(xfit):\n",
    "        xfit = np.concatenate((np.ones(xfit.shape[0], int)[np.newaxis, :].T, xfit), axis=1)\n",
    "        return [w.T@x for x in xfit]\n",
    "        \n",
    "    return predict\n",
    "\n",
    "def linear_regression_gradient_descent(X, y, epochs, learning_rate):\n",
    "    X = np.concatenate((np.ones(X.shape[0], int)[np.newaxis, :].T, X), axis=1)\n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.zeros(n_features, int)\n",
    "\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in indices:\n",
    "            e = y[i] - np.dot(X[i], w)\n",
    "            w = w + learning_rate * e * X[i]\n",
    "\n",
    "    def predict(xfit):\n",
    "        xfit = np.concatenate((np.ones(xfit.shape[0], int)[np.newaxis, :].T, xfit), axis=1)\n",
    "        return [w.T@x for x in xfit]\n",
    "    \n",
    "    return predict\n",
    "\n",
    "def get_mse(X, y, model):\n",
    "    predictions = model(X)\n",
    "    return np.mean((y - predictions)**2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, generate datasets to test your implementations. Use 1-dimensional datasets to visualize the functions estimated by each of your implementations and higher-dimensional datasets to gather numerical data for the validation errors and training times. (9 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "DOMAIN = 10\n",
    "\n",
    "#1d test\n",
    "x = DOMAIN*rng.rand(50)\n",
    "y = -3*x - 15 + 3*rng.rand(50)\n",
    "plt.scatter(x, y)\n",
    "\n",
    "#Test Closed Form\n",
    "model = linear_regression_closed_form(x[:, np.newaxis], y)\n",
    "\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "yfit = model(xfit[:, np.newaxis])\n",
    "\n",
    "plt.plot(xfit, yfit)\n",
    "\n",
    "#Test Stochastic Gradient Descent\n",
    "model = linear_regression_gradient_descent(x[:, np.newaxis], y, 1000, 0.001)\n",
    "\n",
    "xfit = np.linspace(0, DOMAIN, 1000)\n",
    "yfit = model(xfit[:, np.newaxis])\n",
    "\n",
    "plt.plot(xfit, yfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "#many dimension test\n",
    "x = 10*rng.rand(1000, 100)\n",
    "y = 0.1*rng.rand(100) @ x.T + 15 + 2*rng.rand(1000)\n",
    "\n",
    "#Test Closed Form\n",
    "start_time = time.time()\n",
    "model = linear_regression_closed_form(x, y)\n",
    "end_time = time.time()\n",
    "elapsed_time_closed_form = end_time - start_time\n",
    "print(f\"Closed form took: {elapsed_time_closed_form} seconds with mean squared error: {get_mse(x, y, model)}\")\n",
    "\n",
    "#Test Stochastic Gradient Descent\n",
    "start_time = time.time()\n",
    "model = linear_regression_gradient_descent(x, y, 50, 0.0001)\n",
    "end_time = time.time()\n",
    "elapsed_time_closed_form = end_time - start_time\n",
    "print(f\"Closed form took: {elapsed_time_closed_form} seconds with mean squared error: {get_mse(x, y, model)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Nonlinear Regression (17 marks total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using whichever of your linear regresion implementations you found was better in part 1, implement nonlinear regression using Gaussian RBF's. There are many hyperparameters here, so you should use some reasonable deterministic method to choose some of them and use k-fold validation to search for the remaining hyperparameters. Again, generate datasets to test your implementation. Use 1-dimensional datasets to visualize the function predicted by your implementation and use higher-dimensional datasets to gather numerical data for validation errors and training times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 6 marks for nonlinear regression implementation\n",
    "- 6 marks for hyperparameter selection implementation\n",
    "- 4 marks for testing performance (visualization and numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "\n",
    "def rbf_kernel(X, centers, width):\n",
    "    n_x = X.shape[0]\n",
    "    n_centers = centers.shape[0]\n",
    "    \n",
    "    # Initialize an empty matrix to store kernel values\n",
    "    kernel_matrix = np.zeros((n_x, n_centers))\n",
    "    \n",
    "    # Compute pairwise distances and apply RBF kernel\n",
    "    for i in range(n_x):\n",
    "        for j in range(n_centers):\n",
    "            distance = np.linalg.norm(X[i] - centers[j])\n",
    "            kernel_matrix[i, j] = np.exp(-(distance**2) / (width**2))\n",
    "    \n",
    "    return kernel_matrix\n",
    "\n",
    "#shuffles the indices of X and divides them into k folds, returning one fold as train_indices and the rest as val_indices every time it is called\n",
    "def k_fold_cross_validation(X, k = 10):\n",
    "    indices = np.arange(X.shape[0])\n",
    "    rng.shuffle(indices)\n",
    "\n",
    "    folds = np.array_split(indices, k)\n",
    "    for i in range(k):\n",
    "        val_indices = folds[i]\n",
    "        train_indices = np.concatenate([folds[j] for j in range(k) if j != i])\n",
    "        yield val_indices, train_indices\n",
    "\n",
    "#trains a model on dataset X and t. domain is the domain of X in all dimensions.\n",
    "#returns a predict function that, when called with a dataset, returns the y predictions of the model\n",
    "def gaussian_rbf_regression(X, t, domain):\n",
    "    def train_model(X, t, domain, width, rbf_count_multiplier):\n",
    "        num_rbfs = int(X.shape[0]*rbf_count_multiplier)\n",
    "        linear_model = linear_regression_closed_form(X, t)\n",
    "        new_t = t - linear_model(X)\n",
    "    \n",
    "        centers = np.linspace(0, domain, num_rbfs)\n",
    "        gaussian_weights = np.linalg.pinv(rbf_kernel(X, centers, width)) @ new_t\n",
    "    \n",
    "        def predict(xfit):\n",
    "            yfit = linear_model(xfit)\n",
    "        \n",
    "            yfit += gaussian_weights @ rbf_kernel(xfit, centers, width).T\n",
    "            \n",
    "            return yfit\n",
    "        \n",
    "        return predict\n",
    "        \n",
    "    #Find optimal width and number of rbfs with k-fold cross validation\n",
    "    widths = np.linspace(0.1, 2, 10)\n",
    "    rbf_count_multipliers = np.linspace(0.1, 1, 10) #This multiplied by the number of samples rounded to an int gives the number of rbfs\n",
    "    num_folds = 4\n",
    "\n",
    "    mse_scores = []\n",
    "    \n",
    "    for width in widths:\n",
    "        rbf_count_multiplier_mse_scores = []\n",
    "        for rbf_count_multiplier in rbf_count_multipliers:\n",
    "            fold_mse_scores = []\n",
    "            \n",
    "            for train_index, val_index in k_fold_cross_validation(X, num_folds):\n",
    "                X_train, X_val = X[train_index], X[val_index]\n",
    "                y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "                model = train_model(X_train, y_train, domain, width, rbf_count_multiplier)\n",
    "                mse = get_mse(X_val, y_val, model)\n",
    "                fold_mse_scores.append(mse)\n",
    "\n",
    "            avg_mse = np.mean(fold_mse_scores)\n",
    "            rbf_count_multiplier_mse_scores.append(avg_mse)\n",
    "\n",
    "        mse_scores.append(rbf_count_multiplier_mse_scores)\n",
    "\n",
    "    mse_scores = np.array(mse_scores)\n",
    "    optimal_width_index, optimal_rbf_count_multiplier_index = np.unravel_index(np.argmin(mse_scores, axis=None), mse_scores.shape)\n",
    "    optimal_rbf_count_multiplier = rbf_count_multipliers[optimal_rbf_count_multiplier_index]\n",
    "    optimal_width = widths[optimal_width_index]\n",
    "    \n",
    "    model = train_model(X, t, domain, optimal_width, optimal_rbf_count_multiplier)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(9)\n",
    "\n",
    "#1D Test\n",
    "DOMAIN = 20\n",
    "n = 200\n",
    "dim = 1\n",
    "\n",
    "num_rbfs = 5\n",
    "centers = DOMAIN*rng.rand(num_rbfs, dim)\n",
    "width = 1\n",
    "gaussian_weights = 10*rng.rand(num_rbfs) - 5\n",
    "\n",
    "x = DOMAIN*rng.rand(n)\n",
    "y = x/2 + 2*rng.rand(n) + gaussian_weights @ rbf_kernel(x[:, np.newaxis], centers, width).T\n",
    "\n",
    "model = gaussian_rbf_regression(x[:, np.newaxis], y, DOMAIN)\n",
    "plt.scatter(x, y);\n",
    "xfit = np.linspace(min(x), max(x), 1000)\n",
    "yfit = model(xfit[:, np.newaxis])\n",
    "plt.plot(xfit, yfit)\n",
    "print(f\"Mean Squared Error: {get_mse(x[:, np.newaxis], y, model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "#Many Dimension Test (this takes about 70 seconds)\n",
    "DOMAIN = 20\n",
    "n = 500\n",
    "dim = 50\n",
    "\n",
    "num_rbfs = 10\n",
    "centers = DOMAIN*rng.rand(num_rbfs, dim)\n",
    "width = 5*rng.rand()\n",
    "gaussian_weights = 10*rng.rand(num_rbfs) - 5\n",
    "\n",
    "x = 10*rng.rand(n, dim)\n",
    "y = 0.1*rng.rand(dim) @ x.T + 15 + 2*rng.rand(n) + gaussian_weights @ rbf_kernel(x, centers, width).T\n",
    "\n",
    "start_time = time.time()\n",
    "model = gaussian_rbf_regression(x, y, DOMAIN)\n",
    "end_time = time.time()\n",
    "elapsed_time_closed_form = end_time - start_time\n",
    "print(f\"Closed form took: {elapsed_time_closed_form} seconds with mean squared error: {get_mse(x, y, model)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well do you think your implementation is performing? Why? Answer in the markdown cell below, referring to the results of your tests above. (1 mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With mean squared errors of around 0.2, I think my implementation is fairly accurate but based on the visual representation of the one dimensional test, I think my basis functions are overfitting slightly. My implementation is slower than I would have liked, and I think this is coming from the nested loops in the rbf kernel function. I initially had an implementation that used exclusively matrix operations and it performed much faster on the one dimensional test, but I wasn't able to get it to work with multidimensional data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
